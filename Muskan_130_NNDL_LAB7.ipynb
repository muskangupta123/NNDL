{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Import statements"
      ],
      "metadata": {
        "id": "PHNLT07AAgDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense"
      ],
      "metadata": {
        "id": "_IWdUr6PAjBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Dataset Preparation:"
      ],
      "metadata": {
        "id": "CUxm8MQ4DS-D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Import Dataset"
      ],
      "metadata": {
        "id": "Ev-LojaFBAQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "# Mount Google Drive to access files\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load the dataset from Google Drive\n",
        "dataset = pd.read_csv('/content/drive/MyDrive/PoetryFoundationData.csv')\n",
        "\n",
        "# Remove duplicate rows based on the 'Poem' column\n",
        "dataset = dataset.drop_duplicates(subset=['Poem'])\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "dataset.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "aXBa74q6BCLu",
        "outputId": "3e211503-ff6b-4b6b-bafa-88d8805fbd2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0                                              Title  \\\n",
              "0           0  \\r\\r\\n                    Objects Used to Prop...   \n",
              "1           1  \\r\\r\\n                    The New Church\\r\\r\\n...   \n",
              "2           2  \\r\\r\\n                    Look for Me\\r\\r\\n   ...   \n",
              "3           3  \\r\\r\\n                    Wild Life\\r\\r\\n     ...   \n",
              "4           4  \\r\\r\\n                    Umbrella\\r\\r\\n      ...   \n",
              "\n",
              "                                                Poem              Poet Tags  \n",
              "0  \\r\\r\\nDog bone, stapler,\\r\\r\\ncribbage board, ...  Michelle Menting  NaN  \n",
              "1  \\r\\r\\nThe old cupola glinted above the clouds,...     Lucia Cherciu  NaN  \n",
              "2  \\r\\r\\nLook for me under the hood\\r\\r\\nof that ...        Ted Kooser  NaN  \n",
              "3  \\r\\r\\nBehind the silo, the Mother Rabbit\\r\\r\\n...   Grace Cavalieri  NaN  \n",
              "4  \\r\\r\\nWhen I push your button\\r\\r\\nyou fly off...      Connie Wanek  NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cab17b8f-160a-4af3-83b3-59b6fd40b3a3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Title</th>\n",
              "      <th>Poem</th>\n",
              "      <th>Poet</th>\n",
              "      <th>Tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>\\r\\r\\n                    Objects Used to Prop...</td>\n",
              "      <td>\\r\\r\\nDog bone, stapler,\\r\\r\\ncribbage board, ...</td>\n",
              "      <td>Michelle Menting</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>\\r\\r\\n                    The New Church\\r\\r\\n...</td>\n",
              "      <td>\\r\\r\\nThe old cupola glinted above the clouds,...</td>\n",
              "      <td>Lucia Cherciu</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>\\r\\r\\n                    Look for Me\\r\\r\\n   ...</td>\n",
              "      <td>\\r\\r\\nLook for me under the hood\\r\\r\\nof that ...</td>\n",
              "      <td>Ted Kooser</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>\\r\\r\\n                    Wild Life\\r\\r\\n     ...</td>\n",
              "      <td>\\r\\r\\nBehind the silo, the Mother Rabbit\\r\\r\\n...</td>\n",
              "      <td>Grace Cavalieri</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>\\r\\r\\n                    Umbrella\\r\\r\\n      ...</td>\n",
              "      <td>\\r\\r\\nWhen I push your button\\r\\r\\nyou fly off...</td>\n",
              "      <td>Connie Wanek</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cab17b8f-160a-4af3-83b3-59b6fd40b3a3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-cab17b8f-160a-4af3-83b3-59b6fd40b3a3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-cab17b8f-160a-4af3-83b3-59b6fd40b3a3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-532fd49e-aa53-4b74-acc8-a39c934d8bc4\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-532fd49e-aa53-4b74-acc8-a39c934d8bc4')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-532fd49e-aa53-4b74-acc8-a39c934d8bc4 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "dataset",
              "summary": "{\n  \"name\": \"dataset\",\n  \"rows\": 13754,\n  \"fields\": [\n    {\n      \"column\": \"Unnamed: 0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 57,\n        \"min\": 0,\n        \"max\": 199,\n        \"num_unique_values\": 200,\n        \"samples\": [\n          95,\n          15,\n          30\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 13154,\n        \"samples\": [\n          \"\\r\\r\\n                    Step Father\\r\\r\\n                \",\n          \"\\r\\r\\n                    Sabbath lie\\r\\r\\n                \",\n          \"\\r\\r\\n                    The Truth is Concrete\\r\\r\\n                \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Poem\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 13754,\n        \"samples\": [\n          \"\\r\\r\\n\\r\\r\\n\\r\\r\\n\\r\\r\\n(To Saul Touster)\\r\\r\\n\\r\\r\\n\\r\\r\\n\\r\\r\\nI. January 22, 1932\\r\\r\\n \\r\\r\\nCould a four-year-old look out of a square sedan\\r\\r\\n(A Studebaker Six in currency green\\r\\r\\nWith wooden artillery wheels) and see a scene\\r\\r\\nOf snow, light lavender, landing on deepening blue\\r\\r\\nBuildings built out of red-violet bricks, and black\\r\\r\\nPassersby passing by over the widening white\\r\\r\\nStreets darkening blue, under a thickening white\\r\\r\\nSky suddenly undergoing sheer twilight,\\r\\r\\nAnd the yellow but whitening streetlights coming on,\\r\\r\\nAnd remember it now, though the likelihood is gone\\r\\r\\nThat it ever happened at all, and the Village is gone\\r\\r\\nThat it ever could happen in? Memory, guttering out,\\r\\r\\nApparently, finally flares up and banishes doubt.\\r\\r\\n \\r\\r\\n \\r\\r\\nII. May 29, 1941\\r\\r\\n \\r\\r\\nTring. Bells\\r\\r\\nOn grocers\\u2019 boys\\u2019 bicycles ring,\\r\\r\\nFollowed, on cue,\\r\\r\\nBy the jaunty one-note of prayers at two\\r\\r\\nNear churches; taxi horns, a-hunt,\\r\\r\\nCome in for treble; next, the tickety bass\\r\\r\\nOf chain-driven Diamond T\\u2019s, gone elephantine\\r\\r\\nAnd stove-enamelled conifer green\\r\\r\\nDown Greenwich Avenue.\\r\\r\\nOut of the Earle\\r\\r\\nI issue at half-past thirteen,\\r\\r\\nStruck, like a floral clock,\\r\\r\\nBy seasonal\\r\\r\\nManifestations: unreasonable\\r\\r\\nN.Y.U. girls out in their bobby socks\\r\\r\\nAnd rayon blouses; meek boys with their books\\r\\r\\nWho have already moulted mackinaws;Desarrolimiento of\\r\\r\\nNew chrome-green leaves; a rose,\\r\\r\\nGot, blooming, out of bed; and Mrs. Roos-\\r\\r\\nEvelt and Sarah Delano\\r\\r\\nDescending the front stoop of a Jamesian\\r\\r\\nHouse facing south against the Square, the sun\\u2014\\r\\r\\nWho, curveting, his half course not yet run,\\r\\r\\nInfects the earth with crescence;\\r\\r\\nAnd the presence\\r\\r\\nOf process, seen in un-top-hatted,\\r\\r\\nUn-frock-coated burghers and their sons\\r\\r\\nAnd daughters, taking over\\r\\r\\nAll title, right, and interest soever\\r\\r\\nIn this, now their\\r\\r\\nProperty, Washington Square.\\r\\r\\n \\r\\r\\n \\r\\r\\nIII. December 29, 1949\\r\\r\\n \\r\\r\\nThe Hotel Storia ascends\\r\\r\\nAbove me and my new wife; ends\\r\\r\\nEight stories of decline, despair,\\r\\r\\nIron beds and hand-washed underwear\\r\\r\\nAbove us and our leatherette\\r\\r\\nChattels, still grounded on the wet\\r\\r\\nGrey tessellated lobby floor.\\r\\r\\nSoon, through a dingy, numbered door,\\r\\r\\nWe\\u2019ll enter into our new home,\\r\\r\\nProvincials in Imperial Rome\\r\\r\\nTo seek their fortune, or, at least,\\r\\r\\nTo find a job. The wedding feast,\\r\\r\\nDigested and metabolized,\\r\\r\\nDiminishes in idealized\\r\\r\\nGroup photographs, and hard today\\r\\r\\nShunts us together and at bay.\\r\\r\\nOutside the soot-webbed window, sleet\\r\\r\\nScourges the vista of Eighth Street;\\r\\r\\nInside, the radiators clack\\r\\r\\nAnd talk and tell us to go back\\r\\r\\nWhere we came from. A lone pecan\\r\\r\\nFalls from our lunch, a sticky bun,\\r\\r\\nAnd bounces on the trampoline\\r\\r\\nOf the torn bedspread. In the mean\\r\\r\\nDistance of winter, a man sighs,\\r\\r\\nA bedstead creaks, a woman cries.\\r\\r\\n \\r\\r\\n \\r\\r\\nIV. July 14, 1951\\r\\r\\n \\r\\r\\nA summer lull arrives in the West Village,\\r\\r\\nTransmuting houses into silent salvage\\r\\r\\nOf the last century, streets into wreckage\\r\\r\\nUncalled-for by do-gooders who police\\r\\r\\nThe moderniqueness of our ways, patrol\\r\\r\\nThe sanitation of the urban soul.\\r\\r\\nWhat I mean is, devoid of people, all\\r\\r\\nOur dwellings freeze and rust in desuetude,\\r\\r\\nFur over with untenancy, glaze grey\\r\\r\\nWith summer\\u2019s dust and incivility,\\r\\r\\nWith lack of language and engagement, while\\r\\r\\nTheir occupants sport, mutate, and transform\\r\\r\\nThemselves, play at dissembling the god Norm\\r\\r\\nFrom forward bases at Fire Island. But\\u2014\\r\\r\\nException proving rules, dissolving doubt\\u2014\\r\\r\\nYoung Gordon Walker, fledgling editor,\\r\\r\\nMy daylong colleague in the corridors\\r\\r\\nOf Power & Leicht, the trade-book publishers,\\r\\r\\nIs at home to the residue in his\\r\\r\\nAcute apartment in an angle of\\r\\r\\nAbingdon Square. And they\\u2019re all there, the rear-\\r\\r\\nGuard of the garrison of Fort New York:\\r\\r\\nThe skeleton defense of skinny girls\\r\\r\\nWho tap the typewriters of summertime;\\r\\r\\nThe pale male workers who know no time off\\r\\r\\nBecause too recently employed; the old\\r\\r\\nManhattan hands, in patched and gin-stained tweeds;\\r\\r\\nThe writers (Walker\\u2019s one), who see in their\\r\\r\\nCity as desert an oasis of\\r\\r\\nSilence and time to execute their plots\\r\\r\\nAgainst the state of things, but fall a prey\\r\\r\\nTo day succeeding day alone, and call\\r\\r\\nA party to restore themselves to all\\r\\r\\nThe inside jokes of winter, in whose caul\\r\\r\\nPeople click, kiss like billiard balls, and fall,\\r\\r\\nInsensible, into odd pockets. Dense\\r\\r\\nAs gander-feather winter snow, intense\\r\\r\\nAs inextinguishable summer sun\\r\\r\\nAt five o\\u2019clock (which it now is), the noise\\r\\r\\nOf Walker\\u2019s congeries of girls and boys\\r\\r\\nForegathered in their gabbling gratitude\\r\\r\\nStrikes down the stairwell from the altitude\\r\\r\\nOf his wide-open walk-up, beckoning\\r\\r\\nMe, solo, wife gone north, to sickening\\r\\r\\nTop-story heat and talk jackhammering\\r\\r\\nUpon the anvils of all ears. \\u201cChrist, Lou, you\\u2019re here,\\u201d\\r\\r\\nWhoops Walker, topping up a jelly jar\\r\\r\\n(\\u201cCrabapple,\\u201d says the label, still stuck on)\\r\\r\\nWith gin and tonic, a blue liquid smoke\\r\\r\\nThat seeks its level in my unexplored\\r\\r\\nInterior, and sends back a sonar ping\\r\\r\\nTo echo in my head. Two more blue gins.\\r\\r\\nThe sweat that mists my glasses interdicts\\r\\r\\nMy sizing up my interlocutor,\\r\\r\\nWho is, I think, the girl who lives next door,\\r\\r\\nA long-necked, fiddleheaded, celliform\\r\\r\\nGirl cellist propped on an improbably\\r\\r\\nSlim leg. Gin pings are now continuous.\\r\\r\\nThe room swings in its gimbals. In the bath\\r\\r\\nIs silence, blessed, relative, untorn\\r\\r\\nBy the cool drizzle of the bathtub tap,\\r\\r\\nA clear and present invitation. Like\\r\\r\\nA climber conquering K.28,\\r\\r\\nI clamber over the white porcelain\\r\\r\\nRock face, through whitish veils of rubberized\\r\\r\\nShower curtain, and at length, full-dressed, recline\\r\\r\\nIn the encaustic crater, where a fine\\r\\r\\nThread of cold water irrigates my feet,\\r\\r\\nTo sleep, perchance to dream of winter in\\r\\r\\nThe Village, fat with its full complement\\r\\r\\nOf refugees returned to their own turf\\u2014\\r\\r\\nUnspringy as it is\\u2014in a strong surf\\r\\r\\nOf retrogressing lemmings, faces fixed\\r\\r\\nOn the unlovely birthplace of their mixed\\r\\r\\nEmotions, marriages, media, and met-\\r\\r\\nAphors. Lord God of hosts, be with them yet.\\r\\r\\n\",\n          \"\\r\\r\\nEverything that was young went quickly,the way his eyes met mine as soon as wewoke together in a room outside Nanjing,feeling as if all the things that were fallingwould fall and make their thunder, leaveus with the challenge of being happy,all the things that felt given when giftswere not just surprises, but what weknew, what we hoped to take with usto heaven, unbound by faults and sins,not deceived the way we were whenthe end came to what we knew of China,landing me here. I am a wish in the skiesspun out from celestial space to be poor,to be covered with black skin, a feltquilt of a map with only one way to China\\u2009\\u2014\\u2009through pain as big as hogs squealingat killing time on black farms in Alabama\\u2009\\u2014\\u2009the noise of death, the shrill needlethat turns clouds over to rip the airabove the cities where people are youngand all that is given is never taken away.\\r\\r\\n\",\n          \"\\r\\r\\nI dreamt last night\\r\\r\\nthe fright was over, that\\r\\r\\nthe dust came, and then water,   \\r\\r\\nand women and men, together   \\r\\r\\nagain, and all was quiet\\r\\r\\nin the dim moon\\u2019s light.\\r\\r\\nA paean of such patience\\u2014\\r\\r\\nlaughing, laughing at me,   \\r\\r\\nand the days extend over\\r\\r\\nthe earth\\u2019s great cover,\\r\\r\\ngrass, trees, and flower-\\r\\r\\ning season, for no clear reason.\\r\\r\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Poet\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3104,\n        \"samples\": [\n          \"Ouyang Jianghe\",\n          \"John Lydgate\",\n          \"Caitlin Doyle\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Tags\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8283,\n        \"samples\": [\n          \"Living,The Mind,Arts & Sciences,Philosophy,Social Commentaries\",\n          \"Living,Death,Health & Illness,The Body,Love,Social Commentaries\",\n          \"Living,Growing Old,Life Choices,Parenthood,Time & Brevity,Activities,Travels & Journeys,Relationships,Family & Ancestors\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataset Columns"
      ],
      "metadata": {
        "id": "34-NcLatDho_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TG0d1W_lDjWo",
        "outputId": "2b1c1ece-68bf-439c-80a8-dab1aabab2f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 13754 entries, 0 to 13833\n",
            "Data columns (total 5 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   Unnamed: 0  13754 non-null  int64 \n",
            " 1   Title       13754 non-null  object\n",
            " 2   Poem        13754 non-null  object\n",
            " 3   Poet        13754 non-null  object\n",
            " 4   Tags        12854 non-null  object\n",
            "dtypes: int64(1), object(4)\n",
            "memory usage: 644.7+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Concatenate multiple poems into a single text corpus, separating them by newline characters for clarity"
      ],
      "metadata": {
        "id": "2JhEr9aJEGnB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned = dataset.dropna(subset=['Tags'])\n",
        "df_cleaned = df_cleaned.drop_duplicates(subset=['Poem'])\n",
        "\n",
        "corpus = \"\\n\".join(df_cleaned['Poem'].head(100).values)\n",
        "# print(corpus)"
      ],
      "metadata": {
        "id": "g1IYosOSEH4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Data Preprocessing:"
      ],
      "metadata": {
        "id": "FQI1cCF7FWN6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Convert the text to lowercase and remove special characters or punctuation if necessary.  "
      ],
      "metadata": {
        "id": "-jj7HnbmFT9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text = corpus.lower()\n",
        "\n",
        "cleaned_text = re.sub(r'[^a-zA-Z\\s,\\'\"-.]', '', cleaned_text)\n",
        "cleaned_text = re.sub(r'\\r\\r\\n\\n\\r\\r\\n', '\\n', cleaned_text)\n",
        "cleaned_text = re.sub(r'[\\r\\r]', '', cleaned_text)\n",
        "cleaned_text = re.sub(r'\\n \\n', '\\n\\n', cleaned_text)\n",
        "\n",
        "cleaned_text = re.sub(r'\\n{2,}', '\\n\\n', cleaned_text)\n",
        "cleaned_text = re.sub(r' +', ' ', cleaned_text)\n",
        "cleaned_text = cleaned_text.strip()\n",
        "\n",
        "print(cleaned_text[:5000])\n",
        "print(\"============\")\n",
        "print(repr(cleaned_text[:5000]))\n",
        "\n",
        "corpus = cleaned_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUVONYkFFfW-",
        "outputId": "b209e053-1f7a-4a8c-e28b-b4782874dcb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "invisible fish swim this ghost ocean now described by waves of sand, by water-worn rock. soon the fish will learn to walk. then humans will come ashore and paint dreams on the dying stone. then later, much later, the ocean floor will be punctuated by chevy trucks, carrying the dreamers decendants, who are going to the store.\n",
            "dont bother the earth spirit who lives here. she is working on a story. it is the oldest story in the world and it is delicate, changing. if she sees you watching she will invite you in for coffee, give you warm bread, and you will be obligated to stay and listen. but this is no ordinary story. you will have to endure earthquakes, lightning, the deaths of all those you love, the most blinding beauty. its a story so compelling you may never want to leave this is how she traps you. see that stone finger over there that is the only one who ever escaped.\n",
            "hour in which i consider hydrangea, a salt or sand plant, varietal, the question of varietals, the diet of every mother i know, pounds feels like , i have lost i have lost, yes, a sense of my own possible beauty, grown external, i externalize beauty. beauty occurs on the surface of plants the sun darkens the skin of my child, he is so small, he is beautiful (i can see it is obvious) and everything about him is beautiful. his hand swells from the bite spread of some insects venom because he is small. he appears to feel nothing. he smashes his skull against the floor. he screams. i hold him in my lap on the kitchen floor in front of an open freezer, pressing a pack of frozen clay against his forehead. he likes the cold. i see it is so obvious. hydrangea. when i move, when i walk pushing my childs stroller (it is both walking and pushing or hauling, sometimes, also, lifting it is having another body, an adjunct body composed of errand and weight and tenderness and no small amount of power), i imagine i can feel this small amount of weight, this pounds like , interfering with the twitch of every muscle in my body. as an object, a mother is confusing, a middle-aged mother with little spare flesh, i feel every inch of major muscle pulling against gravity and against the weight of my child, now sleeping. this is the hour for thinking hydrangea. let no man look at me. i stop to brush the drowsy childs little eye. his face. he barely considers his mother. i am all around him. why should he consider what is all around him perhaps what is missing is a subtle power of differentiation. i am in, therefore, a time of mass apprehensions.\n",
            "my fathers body is a map\n",
            "a record of his journey\n",
            "\n",
            "he carries a bullet\n",
            "lodged in his left thigh\n",
            "there is a hollow where it entered\n",
            "a protruding bump where it sleeps\n",
            "the doctors say it will never awaken\n",
            "\n",
            "it is the one souvenir he insists on keeping\n",
            "mother has her own opinionsb ca con inyour father is crazy\n",
            "\n",
            "as a child\n",
            "i wanted a scar just like my fathers\n",
            "bold and appalling a mushroom explosion\n",
            "that said i too was at war\n",
            "instead i settled for a grain of rice\n",
            "a scar so small look closely there\n",
            "here between the eyes\n",
            "a bit to the right\n",
            "there on the bridge of my nose\n",
            "\n",
            "father says i was too young to remember\n",
            "it happened while i was sleeping\n",
            "leaking roof the pounding rain\n",
            "drop after drop after drop\n",
            "\n",
            "it has long been forgotten this practice of the mother\n",
            "weaning a child she crushes the seeds of a green\n",
            "chili rubs it to her nipple what the child feels\n",
            "she too will share in this act of love\n",
            "my own mother says it was not meant\n",
            "to be cruel when cruelty she tells me\n",
            "is a childs lips torn from breast as proof\n",
            "back home the women wear teeth marks\n",
            "\n",
            "why are you still seventeen\n",
            "and drifting like a dog after dark,\n",
            "dragging a shadow youve found\n",
            "\n",
            "put it back where it belongs,\n",
            "and that bend of river, too. thats not the road\n",
            "you want, though you have it to yourself.\n",
            "\n",
            "gone are the cars that crawl to town\n",
            "from the reactors, a parade of insects, metallic,\n",
            "fuming along the one four-lane street.\n",
            "\n",
            "the poplars of the shelterbelt lean away\n",
            "from the bypass that never had much to pass by\n",
            "but coyote and rabbitbrush.\n",
            "\n",
            "pinpricks stabbed in a map too dark to read\n",
            "i stared at stars light-years away.\n",
            "listen. that hissing just a sprinkler\n",
            "\n",
            "damping down yesterday until its today.\n",
            "the cottonwoods shiver, or i do,\n",
            "every leaf rustling as if its the one\n",
            "\n",
            "about to tear itself, not i.\n",
            "memory takes the graveyard shift.\n",
            "\n",
            "yes, your childhood now a legend of fountains\n",
            " jorge gulln\n",
            "\n",
            "yes, your childhood, now a legend\n",
            "gone to weeds, still remembers the gray road\n",
            "that set out to cross the desert of the future.\n",
            "\n",
            "and how, always just ahead,\n",
            "gray water glittered, happy to be just a mirage.\n",
            "who steps off the gray bus at the depot\n",
            "\n",
            "sidewalks shudder all the way home.\n",
            "blinds close their scratchy eyes.\n",
            "who settles in your old room\n",
            "\n",
            "sniffy air sprawls as if it owns the place,\n",
            "and now your teenage secrets have no one to tell.\n",
            "for the spider laying claim to the corner,\n",
            "\n",
            "there is a stickiness to spin, that the living may beg\n",
            "to be wrapped in silk and devoured,\n",
            "leaving not even the flinch f\n",
            "============\n",
            "'invisible fish swim this ghost ocean now described by waves of sand, by water-worn rock. soon the fish will learn to walk. then humans will come ashore and paint dreams on the dying stone. then later, much later, the ocean floor will be punctuated by chevy trucks, carrying the dreamers decendants, who are going to the store.\\ndont bother the earth spirit who lives here. she is working on a story. it is the oldest story in the world and it is delicate, changing. if she sees you watching she will invite you in for coffee, give you warm bread, and you will be obligated to stay and listen. but this is no ordinary story. you will have to endure earthquakes, lightning, the deaths of all those you love, the most blinding beauty. its a story so compelling you may never want to leave this is how she traps you. see that stone finger over there that is the only one who ever escaped.\\nhour in which i consider hydrangea, a salt or sand plant, varietal, the question of varietals, the diet of every mother i know, pounds feels like , i have lost i have lost, yes, a sense of my own possible beauty, grown external, i externalize beauty. beauty occurs on the surface of plants the sun darkens the skin of my child, he is so small, he is beautiful (i can see it is obvious) and everything about him is beautiful. his hand swells from the bite spread of some insects venom because he is small. he appears to feel nothing. he smashes his skull against the floor. he screams. i hold him in my lap on the kitchen floor in front of an open freezer, pressing a pack of frozen clay against his forehead. he likes the cold. i see it is so obvious. hydrangea. when i move, when i walk pushing my childs stroller (it is both walking and pushing or hauling, sometimes, also, lifting it is having another body, an adjunct body composed of errand and weight and tenderness and no small amount of power), i imagine i can feel this small amount of weight, this pounds like , interfering with the twitch of every muscle in my body. as an object, a mother is confusing, a middle-aged mother with little spare flesh, i feel every inch of major muscle pulling against gravity and against the weight of my child, now sleeping. this is the hour for thinking hydrangea. let no man look at me. i stop to brush the drowsy childs little eye. his face. he barely considers his mother. i am all around him. why should he consider what is all around him perhaps what is missing is a subtle power of differentiation. i am in, therefore, a time of mass apprehensions.\\nmy fathers body is a map\\na record of his journey\\n\\nhe carries a bullet\\nlodged in his left thigh\\nthere is a hollow where it entered\\na protruding bump where it sleeps\\nthe doctors say it will never awaken\\n\\nit is the one souvenir he insists on keeping\\nmother has her own opinionsb ca con inyour father is crazy\\n\\nas a child\\ni wanted a scar just like my fathers\\nbold and appalling a mushroom explosion\\nthat said i too was at war\\ninstead i settled for a grain of rice\\na scar so small look closely there\\nhere between the eyes\\na bit to the right\\nthere on the bridge of my nose\\n\\nfather says i was too young to remember\\nit happened while i was sleeping\\nleaking roof the pounding rain\\ndrop after drop after drop\\n\\nit has long been forgotten this practice of the mother\\nweaning a child she crushes the seeds of a green\\nchili rubs it to her nipple what the child feels\\nshe too will share in this act of love\\nmy own mother says it was not meant\\nto be cruel when cruelty she tells me\\nis a childs lips torn from breast as proof\\nback home the women wear teeth marks\\n\\nwhy are you still seventeen\\nand drifting like a dog after dark,\\ndragging a shadow youve found\\n\\nput it back where it belongs,\\nand that bend of river, too. thats not the road\\nyou want, though you have it to yourself.\\n\\ngone are the cars that crawl to town\\nfrom the reactors, a parade of insects, metallic,\\nfuming along the one four-lane street.\\n\\nthe poplars of the shelterbelt lean away\\nfrom the bypass that never had much to pass by\\nbut coyote and rabbitbrush.\\n\\npinpricks stabbed in a map too dark to read\\ni stared at stars light-years away.\\nlisten. that hissing just a sprinkler\\n\\ndamping down yesterday until its today.\\nthe cottonwoods shiver, or i do,\\nevery leaf rustling as if its the one\\n\\nabout to tear itself, not i.\\nmemory takes the graveyard shift.\\n\\nyes, your childhood now a legend of fountains\\n jorge gulln\\n\\nyes, your childhood, now a legend\\ngone to weeds, still remembers the gray road\\nthat set out to cross the desert of the future.\\n\\nand how, always just ahead,\\ngray water glittered, happy to be just a mirage.\\nwho steps off the gray bus at the depot\\n\\nsidewalks shudder all the way home.\\nblinds close their scratchy eyes.\\nwho settles in your old room\\n\\nsniffy air sprawls as if it owns the place,\\nand now your teenage secrets have no one to tell.\\nfor the spider laying claim to the corner,\\n\\nthere is a stickiness to spin, that the living may beg\\nto be wrapped in silk and devoured,\\nleaving not even the flinch f'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paragraphs = corpus.split(\"\\n\\n\")\n",
        "\n",
        "# Now, split each paragraph into lines (split by single \\n)\n",
        "lines_in_paragraphs = [paragraph.split(\"\\n\") for paragraph in paragraphs]\n",
        "\n",
        "temp_lines_in_paragraphs = []\n",
        "\n",
        "lines = 0\n",
        "for para in lines_in_paragraphs:\n",
        "    temp = []\n",
        "    for line in para:\n",
        "      line = line.strip()\n",
        "      if line == \"\":\n",
        "        continue\n",
        "      temp.append(line)\n",
        "    if len(temp) > 0:\n",
        "      temp_lines_in_paragraphs.append(temp)\n",
        "\n",
        "    lines += len(para)\n",
        "\n",
        "print(f\"Paragraphs: {len(paragraphs)}\")\n",
        "print(f\"Lines: {lines}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jbZDATXx7AY",
        "outputId": "293aada2-34fd-43f4-c79d-123351716fb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraphs: 612\n",
            "Lines: 2790\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenize the text (e.g., convert each word to a unique integer).  "
      ],
      "metadata": {
        "id": "zBFlW6i6GfVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flattened_lines = [line for paragraph in temp_lines_in_paragraphs for line in paragraph]\n",
        "\n",
        "# Tokenize the corpus\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(flattened_lines)\n",
        "\n",
        "# Get the vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1  # Add 1 for padding token\n",
        "print(\"Vocabulary size:\", vocab_size)\n",
        "\n",
        "# Convert the corpus into a sequence of integers (tokens)\n",
        "sequences = tokenizer.texts_to_sequences(flattened_lines)\n",
        "print(sequences[:20])  # Print the first 20 tokenized words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgYXrEhhGegk",
        "outputId": "37c7e864-5dea-41e8-a4f0-ac4693c82040"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 4685\n",
            "[[528, 370, 529, 24, 701, 163, 62, 1013, 36, 530, 3, 371, 36, 121, 1014, 702, 703, 1, 370, 25, 704, 4, 222, 82, 705, 25, 122, 1702, 6, 531, 284, 12, 1, 317, 706, 82, 223, 176, 223, 1, 163, 199, 25, 32, 1015, 36, 1703, 1016, 1704, 1, 1705, 1706, 47, 50, 106, 4, 1, 532], [107, 1707, 1, 135, 1708, 47, 247, 98, 33, 10, 1017, 12, 2, 177, 11, 10, 1, 1709, 177, 5, 1, 99, 6, 11, 10, 1710, 1711, 45, 33, 1712, 9, 533, 33, 25, 1713, 9, 5, 17, 318, 319, 9, 372, 1018, 6, 9, 25, 32, 1714, 4, 430, 6, 373, 30, 24, 10, 46, 1019, 177, 9, 25, 40, 4, 1715, 1716, 534, 1, 707, 3, 37, 224, 9, 123, 1, 374, 1717, 285, 35, 2, 177, 48, 1718, 9, 248, 83, 124, 4, 286, 24, 10, 53, 33, 1719, 9, 85, 13, 706, 708, 90, 56, 13, 10, 1, 94, 31, 47, 200, 1020], [535, 5, 125, 7, 1021, 709, 2, 710, 28, 371, 711, 1720, 1, 536, 3, 1721, 1, 1722, 3, 136, 80, 7, 100, 1022, 537, 14, 7, 40, 154, 7, 40, 154, 201, 2, 431, 3, 8, 137, 712, 285, 538, 1723, 7, 1724, 285, 285, 1023, 12, 1, 713, 3, 1725, 1, 249, 1726, 1, 202, 3, 8, 155, 22, 10, 48, 126, 22, 10, 225, 7, 64, 85, 11, 10, 1024, 6, 138, 78, 75, 10, 225, 16, 139, 1727, 34, 1, 1728, 1025, 3, 116, 1026, 1729, 86, 22, 10, 126, 22, 1027, 4, 149, 150, 22, 1730, 16, 1028, 140, 1, 199, 22, 1731, 7, 203, 75, 5, 8, 539, 12, 1, 540, 199, 5, 375, 3, 38, 320, 1732, 1733, 2, 1734, 3, 541, 1735, 140, 16, 432, 22, 1736, 1, 321, 7, 85, 11, 10, 48, 1024, 709, 49, 7, 1029, 49, 7, 222, 1030, 8, 542, 1737, 11, 10, 204, 543, 6, 1030, 28, 1738, 205, 544, 545, 11, 10, 546, 164, 58, 38, 1739, 58, 1740, 3, 1741, 6, 322, 6, 1742, 6, 46, 126, 1031, 3, 323, 7, 324, 7, 64, 149, 24, 126, 1031, 3, 322, 24, 1022, 14, 1743, 15, 1, 1744, 3, 136, 714, 5, 8, 58, 18, 38, 1032, 2, 80, 10, 1745, 2, 1033, 1746, 80, 15, 190, 1747, 715, 7, 149, 136, 1748, 3, 547, 714, 716, 140, 1749, 6, 140, 1, 322, 3, 8, 155, 62, 376, 24, 10, 1, 535, 17, 717, 709, 91, 46, 72, 141, 27, 29, 7, 226, 4, 1750, 1, 1034, 542, 190, 250, 16, 95, 22, 1035, 1751, 16, 80, 7, 101, 37, 108, 75, 206, 377, 22, 1021, 43, 10, 37, 108, 75, 548, 43, 10, 1036, 10, 2, 1752, 323, 3, 1753, 7, 101, 5, 718, 2, 76, 3, 433, 1754], [8, 434, 58, 10, 2, 178], [2, 1755, 3, 16, 719], [22, 1037, 2, 378], [1756, 5, 16, 179, 1757], [56, 10, 2, 1758, 51, 11, 1038], [2, 1759, 1760, 51, 11, 1039], [1, 1040, 70, 11, 25, 83, 1041], [11, 10, 1, 31, 1761, 22, 1762, 12, 720], [80, 65, 23, 137, 1763, 1764, 1765, 1766, 117, 10, 1767], [18, 2, 155], [7, 207, 2, 721, 102, 14, 8, 434], [1042, 6, 1768, 2, 1769, 1770], [13, 57, 7, 66, 19, 27, 379], [722, 7, 1043, 17, 2, 1771, 3, 723], [2, 721, 48, 126, 141, 1044, 56], [98, 227, 1, 129], [2, 724, 4, 1, 156]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Use a sliding window to create sequences of words for the LSTM model. For example, if n=5, create sequences of 5 words with the 6th word as the target."
      ],
      "metadata": {
        "id": "OAHEJNCiHC_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_length = 5  # Length of the sequence\n",
        "\n",
        "# Create input-output pairs (X, y) based on the sliding window\n",
        "X, y = [], []\n",
        "\n",
        "for line in sequences:\n",
        "    for i in range(sequence_length, len(line)):\n",
        "        X.append(line[i-sequence_length:i])\n",
        "        y.append(line[i])\n",
        "\n",
        "print(\"X shape:\", len(X))\n",
        "print(\"y shape:\", len(y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsdAMmiTHCrQ",
        "outputId": "7b5aebfd-b650-42cb-b5f9-471bf58c5e72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape: 7098\n",
            "y shape: 7098\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pad the sequences so that they all have the same length."
      ],
      "metadata": {
        "id": "wn8KrhXyFkIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = pad_sequences(X, maxlen = sequence_length, padding='pre')\n",
        "y = np.array(y)\n",
        "print(\"Padded X shape:\", X.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DppME3iPHqtL",
        "outputId": "5e837e0e-17d0-4fff-94e1-627b7f4b1bc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Padded X shape: (7098, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. LSTM Model Development"
      ],
      "metadata": {
        "id": "SodivjFaH9i6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  o Define an LSTM model with the following structure:  \n",
        "    - An embedding layer with an appropriate input dimension (based on vocabulary size) and output dimension (e.g., 100).  \n",
        "    - One or two LSTM layers with 100 units each.  \n",
        "    - A dropout layer with a rate of 0.2 to prevent overfitting.  \n",
        "    - A dense output layer with softmax activation for word prediction."
      ],
      "metadata": {
        "id": "dYyJjR3OIJr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=100, input_length=X.shape[1]))\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dropout(0.2))  # Dropout layer to prevent overfitting\n",
        "model.add(Dense(vocab_size, activation='softmax'))  # Output layer for word prediction\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZHw_deWIKdu",
        "outputId": "2ee0df5d-e028-4d26-a6ce-48faad5feca2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 5, 100)            468500    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 5, 100)            80400     \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 100)               80400     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 100)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 4685)              473185    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1102485 (4.21 MB)\n",
            "Trainable params: 1102485 (4.21 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Training:  "
      ],
      "metadata": {
        "id": "AJW296QxJJuh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Compile the model with categorical cross-entropy as the loss function and accuracy as the metric."
      ],
      "metadata": {
        "id": "7ISnvEbqJNZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "# One-hot encode the target labels\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "print(\"y shape after one-hot encoding:\", y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsjT6noXJzG2",
        "outputId": "09d5af14-8e03-401a-9a06-10352ebe8408"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y shape after one-hot encoding: (7098, 4685)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train the model on the sequences for 10-20 epochs (or until it achieves satisfactory performance)."
      ],
      "metadata": {
        "id": "cPZjP6POJJmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(X, y, epochs=10, batch_size=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aJOB686KAbm",
        "outputId": "b78d1efc-b2fb-4a8b-abb1-ebe1e92f2ee2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "111/111 [==============================] - 5s 19ms/step - loss: 7.5300 - accuracy: 0.0533\n",
            "Epoch 2/10\n",
            "111/111 [==============================] - 2s 18ms/step - loss: 6.7217 - accuracy: 0.0609\n",
            "Epoch 3/10\n",
            "111/111 [==============================] - 2s 19ms/step - loss: 6.6194 - accuracy: 0.0609\n",
            "Epoch 4/10\n",
            "111/111 [==============================] - 2s 19ms/step - loss: 6.5517 - accuracy: 0.0609\n",
            "Epoch 5/10\n",
            "111/111 [==============================] - 2s 19ms/step - loss: 6.4548 - accuracy: 0.0609\n",
            "Epoch 6/10\n",
            "111/111 [==============================] - 2s 19ms/step - loss: 6.3262 - accuracy: 0.0609\n",
            "Epoch 7/10\n",
            "111/111 [==============================] - 2s 19ms/step - loss: 6.2342 - accuracy: 0.0614\n",
            "Epoch 8/10\n",
            "111/111 [==============================] - 2s 19ms/step - loss: 6.1572 - accuracy: 0.0668\n",
            "Epoch 9/10\n",
            "111/111 [==============================] - 2s 19ms/step - loss: 6.0525 - accuracy: 0.0727\n",
            "Epoch 10/10\n",
            "111/111 [==============================] - 2s 19ms/step - loss: 5.9259 - accuracy: 0.0741\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x78ba74085f30>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Text Generation:"
      ],
      "metadata": {
        "id": "0O3wtfm3u-W9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(seed_text, next_words, model, tokenizer, max_sequence_len, temperature=1.0, words_per_set=5):\n",
        "    previous_word = \"\"  # Track the previous word to avoid consecutive repetition\n",
        "    word_count = 0  # Track the number of words generated in the current set\n",
        "    for i in range(next_words):\n",
        "        # Tokenize the seed text\n",
        "        tokenized_text = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\n",
        "        # Pad the tokenized sequence\n",
        "        tokenized_text = pad_sequences([tokenized_text], maxlen=max_sequence_len, padding='pre')\n",
        "\n",
        "        # Predict the next word probabilities\n",
        "        predicted = model.predict(tokenized_text, verbose=0)[0]\n",
        "\n",
        "        # Apply temperature to predictions (to make them more diverse)\n",
        "        predicted = np.log(predicted + 1e-7) / temperature\n",
        "        predicted = np.exp(predicted) / np.sum(np.exp(predicted))  # Normalize to get valid probabilities\n",
        "\n",
        "        # Sample the next word based on the adjusted probabilities\n",
        "        predicted_word_idx = np.random.choice(len(predicted), p=predicted)\n",
        "        predicted_word = tokenizer.index_word.get(predicted_word_idx, '')\n",
        "\n",
        "        # Ensure the predicted word is not the same as the previous word\n",
        "        if predicted_word == previous_word:\n",
        "            continue  # Skip the word if it's the same as the previous one\n",
        "\n",
        "        if word_count != 0:\n",
        "          seed_text += \" \"\n",
        "\n",
        "        # Append the predicted word to the seed text\n",
        "        seed_text += predicted_word\n",
        "\n",
        "        # Update the previous word\n",
        "        previous_word = predicted_word\n",
        "\n",
        "        # Track the number of words in the current set\n",
        "        word_count += 1\n",
        "\n",
        "        # If we've reached the words_per_set limit, add a comma and reset word count\n",
        "        if word_count >= words_per_set:\n",
        "            seed_text += \",\\n\"\n",
        "            word_count = 0  # Reset the word count for the next set of words\n",
        "\n",
        "    return seed_text\n",
        "\n",
        "seed = \"once upon a time \"\n",
        "generated_poem = generate_text(seed, next_words=50, model=model, tokenizer=tokenizer, max_sequence_len=X.shape[1], temperature=0.7, words_per_set=5)\n",
        "print(generated_poem)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgE70ZE3OZQO",
        "outputId": "3e37a995-055c-44d6-cca2-255a900e6043"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "once upon a time of say and a wars,\n",
            "of you ambivalence and other,\n",
            "head their better old still,\n",
            "in the knew body what,\n",
            "to me was be this,\n",
            "to alone a just of,\n",
            "the far we do our,\n",
            "craving power a trove of,\n",
            "the affliction and bon the,\n",
            "mouth of is walked\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Evaluation and Experimentation:"
      ],
      "metadata": {
        "id": "wDCWxlGXvf1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the LSTM model\n",
        "model2 = Sequential()\n",
        "model2.add(Embedding(input_dim=vocab_size, output_dim=100, input_length=X.shape[1]))\n",
        "model2.add(LSTM(50, return_sequences=True))\n",
        "model2.add(LSTM(50))\n",
        "model2.add(Dropout(0.4))  # Dropout layer to prevent overfitting\n",
        "model2.add(Dense(vocab_size, activation='softmax'))  # Output layer for word prediction\n",
        "\n",
        "# Compile the model\n",
        "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2qpWhAcvg2E",
        "outputId": "82a6b227-f040-40ac-fbe3-c9f3390a0f5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 5, 100)            468500    \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 5, 50)             30200     \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 50)                20200     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 50)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 4685)              238935    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 757835 (2.89 MB)\n",
            "Trainable params: 757835 (2.89 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model2.fit(X, y, epochs=30, batch_size=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tmwf95hOvoXK",
        "outputId": "9dbfec79-b24a-4eb7-b127-faefb817df17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "111/111 [==============================] - 5s 14ms/step - loss: 7.6568 - accuracy: 0.0538\n",
            "Epoch 2/30\n",
            "111/111 [==============================] - 2s 14ms/step - loss: 6.7824 - accuracy: 0.0609\n",
            "Epoch 3/30\n",
            "111/111 [==============================] - 2s 14ms/step - loss: 6.6630 - accuracy: 0.0609\n",
            "Epoch 4/30\n",
            "111/111 [==============================] - 2s 14ms/step - loss: 6.5983 - accuracy: 0.0609\n",
            "Epoch 5/30\n",
            "111/111 [==============================] - 2s 14ms/step - loss: 6.5303 - accuracy: 0.0609\n",
            "Epoch 6/30\n",
            "111/111 [==============================] - 2s 14ms/step - loss: 6.4455 - accuracy: 0.0609\n",
            "Epoch 7/30\n",
            "111/111 [==============================] - 2s 14ms/step - loss: 6.3483 - accuracy: 0.0609\n",
            "Epoch 8/30\n",
            "111/111 [==============================] - 2s 14ms/step - loss: 6.2845 - accuracy: 0.0610\n",
            "Epoch 9/30\n",
            "111/111 [==============================] - 2s 14ms/step - loss: 6.2367 - accuracy: 0.0611\n",
            "Epoch 10/30\n",
            "111/111 [==============================] - 1s 13ms/step - loss: 6.1914 - accuracy: 0.0609\n",
            "Epoch 11/30\n",
            "111/111 [==============================] - 2s 14ms/step - loss: 6.1563 - accuracy: 0.0610\n",
            "Epoch 12/30\n",
            "111/111 [==============================] - 2s 14ms/step - loss: 6.1152 - accuracy: 0.0624\n",
            "Epoch 13/30\n",
            "111/111 [==============================] - 2s 14ms/step - loss: 6.0638 - accuracy: 0.0633\n",
            "Epoch 14/30\n",
            "111/111 [==============================] - 2s 14ms/step - loss: 6.0131 - accuracy: 0.0626\n",
            "Epoch 15/30\n",
            "111/111 [==============================] - 2s 14ms/step - loss: 5.9459 - accuracy: 0.0668\n",
            "Epoch 16/30\n",
            "111/111 [==============================] - 2s 14ms/step - loss: 5.8729 - accuracy: 0.0686\n",
            "Epoch 17/30\n",
            "111/111 [==============================] - 2s 14ms/step - loss: 5.8086 - accuracy: 0.0711\n",
            "Epoch 18/30\n",
            "111/111 [==============================] - 2s 14ms/step - loss: 5.7446 - accuracy: 0.0737\n",
            "Epoch 19/30\n",
            "111/111 [==============================] - 2s 14ms/step - loss: 5.6805 - accuracy: 0.0752\n",
            "Epoch 20/30\n",
            "111/111 [==============================] - 1s 13ms/step - loss: 5.6209 - accuracy: 0.0762\n",
            "Epoch 21/30\n",
            "111/111 [==============================] - 2s 14ms/step - loss: 5.5547 - accuracy: 0.0776\n",
            "Epoch 22/30\n",
            "111/111 [==============================] - 1s 13ms/step - loss: 5.4939 - accuracy: 0.0827\n",
            "Epoch 23/30\n",
            "111/111 [==============================] - 2s 14ms/step - loss: 5.4411 - accuracy: 0.0868\n",
            "Epoch 24/30\n",
            "111/111 [==============================] - 2s 14ms/step - loss: 5.3797 - accuracy: 0.0859\n",
            "Epoch 25/30\n",
            "111/111 [==============================] - 2s 14ms/step - loss: 5.3259 - accuracy: 0.0869\n",
            "Epoch 26/30\n",
            "111/111 [==============================] - 2s 14ms/step - loss: 5.2682 - accuracy: 0.0920\n",
            "Epoch 27/30\n",
            "111/111 [==============================] - 2s 14ms/step - loss: 5.2160 - accuracy: 0.0940\n",
            "Epoch 28/30\n",
            "111/111 [==============================] - 1s 13ms/step - loss: 5.1631 - accuracy: 0.0965\n",
            "Epoch 29/30\n",
            "111/111 [==============================] - 2s 14ms/step - loss: 5.1143 - accuracy: 0.1026\n",
            "Epoch 30/30\n",
            "111/111 [==============================] - 2s 14ms/step - loss: 5.0515 - accuracy: 0.1027\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x78b94c7a0fd0>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed = \"In a Faraway Land, \"\n",
        "generated_poem = generate_text(seed, next_words=50, model=model2, tokenizer=tokenizer, max_sequence_len=X.shape[1], temperature=0.7, words_per_set=7)\n",
        "print(generated_poem)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkJnkBPWv-Ue",
        "outputId": "f2b3069b-62ea-4bf6-b2a0-59b8daa57f1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In a Faraway Land, wondered nipple you the grave blessd de,\n",
            "slap his hole scrap bowed flares as,\n",
            "they billows until third of rideable shall,\n",
            "around at smoke strong wheelchairs into to,\n",
            "bed as in the blown of too,\n",
            "light of a trucks holding it they,\n",
            "tells of a wheel in the food,\n",
            "out\n"
          ]
        }
      ]
    }
  ]
}